{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c4bfdf",
   "metadata": {},
   "source": [
    "# Analise de credito com rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e99811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier #Importacao da biblioteca perceptron com mais de uma camada\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ea7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Credito.pkl', 'rb') as f: # Carregamento da base de dados\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e635f575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f0037f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdb4bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69413264\n",
      "Iteration 2, loss = 1.67445702\n",
      "Iteration 3, loss = 1.65537663\n",
      "Iteration 4, loss = 1.63685439\n",
      "Iteration 5, loss = 1.61872945\n",
      "Iteration 6, loss = 1.60090233\n",
      "Iteration 7, loss = 1.58386117\n",
      "Iteration 8, loss = 1.56668731\n",
      "Iteration 9, loss = 1.55015990\n",
      "Iteration 10, loss = 1.53381465\n",
      "Iteration 11, loss = 1.51776785\n",
      "Iteration 12, loss = 1.50198072\n",
      "Iteration 13, loss = 1.48635017\n",
      "Iteration 14, loss = 1.47087900\n",
      "Iteration 15, loss = 1.45557990\n",
      "Iteration 16, loss = 1.44021393\n",
      "Iteration 17, loss = 1.42513225\n",
      "Iteration 18, loss = 1.41024221\n",
      "Iteration 19, loss = 1.39602089\n",
      "Iteration 20, loss = 1.38278922\n",
      "Iteration 21, loss = 1.37015396\n",
      "Iteration 22, loss = 1.35789631\n",
      "Iteration 23, loss = 1.34569176\n",
      "Iteration 24, loss = 1.33341208\n",
      "Iteration 25, loss = 1.32108806\n",
      "Iteration 26, loss = 1.30832414\n",
      "Iteration 27, loss = 1.29365351\n",
      "Iteration 28, loss = 1.27798973\n",
      "Iteration 29, loss = 1.25987827\n",
      "Iteration 30, loss = 1.23973111\n",
      "Iteration 31, loss = 1.21681017\n",
      "Iteration 32, loss = 1.19250577\n",
      "Iteration 33, loss = 1.16702640\n",
      "Iteration 34, loss = 1.14066099\n",
      "Iteration 35, loss = 1.11352356\n",
      "Iteration 36, loss = 1.08642606\n",
      "Iteration 37, loss = 1.05940618\n",
      "Iteration 38, loss = 1.03298332\n",
      "Iteration 39, loss = 1.00720278\n",
      "Iteration 40, loss = 0.98234956\n",
      "Iteration 41, loss = 0.95874127\n",
      "Iteration 42, loss = 0.93669843\n",
      "Iteration 43, loss = 0.91593936\n",
      "Iteration 44, loss = 0.89600497\n",
      "Iteration 45, loss = 0.87758168\n",
      "Iteration 46, loss = 0.86007814\n",
      "Iteration 47, loss = 0.84341638\n",
      "Iteration 48, loss = 0.82776526\n",
      "Iteration 49, loss = 0.81276624\n",
      "Iteration 50, loss = 0.79831758\n",
      "Iteration 51, loss = 0.78437633\n",
      "Iteration 52, loss = 0.77030335\n",
      "Iteration 53, loss = 0.75747903\n",
      "Iteration 54, loss = 0.74496682\n",
      "Iteration 55, loss = 0.73302507\n",
      "Iteration 56, loss = 0.72176692\n",
      "Iteration 57, loss = 0.71105816\n",
      "Iteration 58, loss = 0.70066145\n",
      "Iteration 59, loss = 0.69060114\n",
      "Iteration 60, loss = 0.68065090\n",
      "Iteration 61, loss = 0.67115827\n",
      "Iteration 62, loss = 0.66192867\n",
      "Iteration 63, loss = 0.65264495\n",
      "Iteration 64, loss = 0.64357322\n",
      "Iteration 65, loss = 0.63481119\n",
      "Iteration 66, loss = 0.62651314\n",
      "Iteration 67, loss = 0.61862641\n",
      "Iteration 68, loss = 0.61047490\n",
      "Iteration 69, loss = 0.60264889\n",
      "Iteration 70, loss = 0.59518069\n",
      "Iteration 71, loss = 0.58723116\n",
      "Iteration 72, loss = 0.57997500\n",
      "Iteration 73, loss = 0.57294175\n",
      "Iteration 74, loss = 0.56584745\n",
      "Iteration 75, loss = 0.55879782\n",
      "Iteration 76, loss = 0.55136362\n",
      "Iteration 77, loss = 0.54403270\n",
      "Iteration 78, loss = 0.53723206\n",
      "Iteration 79, loss = 0.53002690\n",
      "Iteration 80, loss = 0.52301119\n",
      "Iteration 81, loss = 0.51644852\n",
      "Iteration 82, loss = 0.50947975\n",
      "Iteration 83, loss = 0.50280610\n",
      "Iteration 84, loss = 0.49609367\n",
      "Iteration 85, loss = 0.48940251\n",
      "Iteration 86, loss = 0.48299470\n",
      "Iteration 87, loss = 0.47680568\n",
      "Iteration 88, loss = 0.47090689\n",
      "Iteration 89, loss = 0.46514254\n",
      "Iteration 90, loss = 0.45965080\n",
      "Iteration 91, loss = 0.45452685\n",
      "Iteration 92, loss = 0.44733529\n",
      "Iteration 93, loss = 0.43839813\n",
      "Iteration 94, loss = 0.42918798\n",
      "Iteration 95, loss = 0.42039421\n",
      "Iteration 96, loss = 0.41195078\n",
      "Iteration 97, loss = 0.40432287\n",
      "Iteration 98, loss = 0.39683696\n",
      "Iteration 99, loss = 0.39001363\n",
      "Iteration 100, loss = 0.38363955\n",
      "Iteration 101, loss = 0.37779751\n",
      "Iteration 102, loss = 0.37223479\n",
      "Iteration 103, loss = 0.36717162\n",
      "Iteration 104, loss = 0.36223200\n",
      "Iteration 105, loss = 0.35778166\n",
      "Iteration 106, loss = 0.35371959\n",
      "Iteration 107, loss = 0.34966858\n",
      "Iteration 108, loss = 0.34599808\n",
      "Iteration 109, loss = 0.34231385\n",
      "Iteration 110, loss = 0.33898835\n",
      "Iteration 111, loss = 0.33583411\n",
      "Iteration 112, loss = 0.33276711\n",
      "Iteration 113, loss = 0.32981413\n",
      "Iteration 114, loss = 0.32706051\n",
      "Iteration 115, loss = 0.32428300\n",
      "Iteration 116, loss = 0.32179171\n",
      "Iteration 117, loss = 0.31930368\n",
      "Iteration 118, loss = 0.31682690\n",
      "Iteration 119, loss = 0.31455863\n",
      "Iteration 120, loss = 0.31225576\n",
      "Iteration 121, loss = 0.31004842\n",
      "Iteration 122, loss = 0.30793372\n",
      "Iteration 123, loss = 0.30598569\n",
      "Iteration 124, loss = 0.30398791\n",
      "Iteration 125, loss = 0.30211886\n",
      "Iteration 126, loss = 0.30036868\n",
      "Iteration 127, loss = 0.29864963\n",
      "Iteration 128, loss = 0.29705129\n",
      "Iteration 129, loss = 0.29544601\n",
      "Iteration 130, loss = 0.29397796\n",
      "Iteration 131, loss = 0.29251908\n",
      "Iteration 132, loss = 0.29107331\n",
      "Iteration 133, loss = 0.28973150\n",
      "Iteration 134, loss = 0.28836943\n",
      "Iteration 135, loss = 0.28696780\n",
      "Iteration 136, loss = 0.28570624\n",
      "Iteration 137, loss = 0.28438784\n",
      "Iteration 138, loss = 0.28313931\n",
      "Iteration 139, loss = 0.28196366\n",
      "Iteration 140, loss = 0.28084479\n",
      "Iteration 141, loss = 0.27976850\n",
      "Iteration 142, loss = 0.27868139\n",
      "Iteration 143, loss = 0.27762164\n",
      "Iteration 144, loss = 0.27650419\n",
      "Iteration 145, loss = 0.27537329\n",
      "Iteration 146, loss = 0.27428587\n",
      "Iteration 147, loss = 0.27320596\n",
      "Iteration 148, loss = 0.27214273\n",
      "Iteration 149, loss = 0.27117465\n",
      "Iteration 150, loss = 0.27019933\n",
      "Iteration 151, loss = 0.26917403\n",
      "Iteration 152, loss = 0.26824415\n",
      "Iteration 153, loss = 0.26724688\n",
      "Iteration 154, loss = 0.26630395\n",
      "Iteration 155, loss = 0.26545209\n",
      "Iteration 156, loss = 0.26453524\n",
      "Iteration 157, loss = 0.26369726\n",
      "Iteration 158, loss = 0.26281334\n",
      "Iteration 159, loss = 0.26199516\n",
      "Iteration 160, loss = 0.26117039\n",
      "Iteration 161, loss = 0.26034100\n",
      "Iteration 162, loss = 0.25953823\n",
      "Iteration 163, loss = 0.25875548\n",
      "Iteration 164, loss = 0.25801308\n",
      "Iteration 165, loss = 0.25717113\n",
      "Iteration 166, loss = 0.25623380\n",
      "Iteration 167, loss = 0.25531420\n",
      "Iteration 168, loss = 0.25437429\n",
      "Iteration 169, loss = 0.25339371\n",
      "Iteration 170, loss = 0.25244872\n",
      "Iteration 171, loss = 0.25144708\n",
      "Iteration 172, loss = 0.25047757\n",
      "Iteration 173, loss = 0.24937281\n",
      "Iteration 174, loss = 0.24817973\n",
      "Iteration 175, loss = 0.24697308\n",
      "Iteration 176, loss = 0.24567186\n",
      "Iteration 177, loss = 0.24444400\n",
      "Iteration 178, loss = 0.24313395\n",
      "Iteration 179, loss = 0.24195590\n",
      "Iteration 180, loss = 0.24062194\n",
      "Iteration 181, loss = 0.23926129\n",
      "Iteration 182, loss = 0.23778010\n",
      "Iteration 183, loss = 0.23632072\n",
      "Iteration 184, loss = 0.23498272\n",
      "Iteration 185, loss = 0.23351211\n",
      "Iteration 186, loss = 0.23210738\n",
      "Iteration 187, loss = 0.23073980\n",
      "Iteration 188, loss = 0.22926994\n",
      "Iteration 189, loss = 0.22766675\n",
      "Iteration 190, loss = 0.22624911\n",
      "Iteration 191, loss = 0.22473840\n",
      "Iteration 192, loss = 0.22319278\n",
      "Iteration 193, loss = 0.22169697\n",
      "Iteration 194, loss = 0.22026659\n",
      "Iteration 195, loss = 0.21882250\n",
      "Iteration 196, loss = 0.21745195\n",
      "Iteration 197, loss = 0.21611166\n",
      "Iteration 198, loss = 0.21492591\n",
      "Iteration 199, loss = 0.21376713\n",
      "Iteration 200, loss = 0.21253719\n",
      "Iteration 201, loss = 0.21137204\n",
      "Iteration 202, loss = 0.21026040\n",
      "Iteration 203, loss = 0.20918599\n",
      "Iteration 204, loss = 0.20802515\n",
      "Iteration 205, loss = 0.20685014\n",
      "Iteration 206, loss = 0.20572512\n",
      "Iteration 207, loss = 0.20464252\n",
      "Iteration 208, loss = 0.20357889\n",
      "Iteration 209, loss = 0.20251215\n",
      "Iteration 210, loss = 0.20153474\n",
      "Iteration 211, loss = 0.20051099\n",
      "Iteration 212, loss = 0.19956601\n",
      "Iteration 213, loss = 0.19863583\n",
      "Iteration 214, loss = 0.19762288\n",
      "Iteration 215, loss = 0.19660980\n",
      "Iteration 216, loss = 0.19565007\n",
      "Iteration 217, loss = 0.19461097\n",
      "Iteration 218, loss = 0.19362983\n",
      "Iteration 219, loss = 0.19263863\n",
      "Iteration 220, loss = 0.19171200\n",
      "Iteration 221, loss = 0.19078801\n",
      "Iteration 222, loss = 0.18989605\n",
      "Iteration 223, loss = 0.18899642\n",
      "Iteration 224, loss = 0.18810670\n",
      "Iteration 225, loss = 0.18726423\n",
      "Iteration 226, loss = 0.18633457\n",
      "Iteration 227, loss = 0.18549186\n",
      "Iteration 228, loss = 0.18460364\n",
      "Iteration 229, loss = 0.18372811\n",
      "Iteration 230, loss = 0.18282920\n",
      "Iteration 231, loss = 0.18183619\n",
      "Iteration 232, loss = 0.18092058\n",
      "Iteration 233, loss = 0.17999598\n",
      "Iteration 234, loss = 0.17906985\n",
      "Iteration 235, loss = 0.17813516\n",
      "Iteration 236, loss = 0.17713042\n",
      "Iteration 237, loss = 0.17620347\n",
      "Iteration 238, loss = 0.17516949\n",
      "Iteration 239, loss = 0.17413441\n",
      "Iteration 240, loss = 0.17302052\n",
      "Iteration 241, loss = 0.17192268\n",
      "Iteration 242, loss = 0.17078926\n",
      "Iteration 243, loss = 0.16970844\n",
      "Iteration 244, loss = 0.16860025\n",
      "Iteration 245, loss = 0.16745821\n",
      "Iteration 246, loss = 0.16646429\n",
      "Iteration 247, loss = 0.16551545\n",
      "Iteration 248, loss = 0.16457755\n",
      "Iteration 249, loss = 0.16368313\n",
      "Iteration 250, loss = 0.16275962\n",
      "Iteration 251, loss = 0.16187079\n",
      "Iteration 252, loss = 0.16099555\n",
      "Iteration 253, loss = 0.16008330\n",
      "Iteration 254, loss = 0.15923301\n",
      "Iteration 255, loss = 0.15849520\n",
      "Iteration 256, loss = 0.15762918\n",
      "Iteration 257, loss = 0.15675957\n",
      "Iteration 258, loss = 0.15597771\n",
      "Iteration 259, loss = 0.15514341\n",
      "Iteration 260, loss = 0.15428376\n",
      "Iteration 261, loss = 0.15337914\n",
      "Iteration 262, loss = 0.15248753\n",
      "Iteration 263, loss = 0.15150934\n",
      "Iteration 264, loss = 0.15057382\n",
      "Iteration 265, loss = 0.14965231\n",
      "Iteration 266, loss = 0.14870388\n",
      "Iteration 267, loss = 0.14772017\n",
      "Iteration 268, loss = 0.14658354\n",
      "Iteration 269, loss = 0.14552148\n",
      "Iteration 270, loss = 0.14449147\n",
      "Iteration 271, loss = 0.14340365\n",
      "Iteration 272, loss = 0.14235249\n",
      "Iteration 273, loss = 0.14130183\n",
      "Iteration 274, loss = 0.14031943\n",
      "Iteration 275, loss = 0.13929437\n",
      "Iteration 276, loss = 0.13823083\n",
      "Iteration 277, loss = 0.13720586\n",
      "Iteration 278, loss = 0.13611732\n",
      "Iteration 279, loss = 0.13490369\n",
      "Iteration 280, loss = 0.13364704\n",
      "Iteration 281, loss = 0.13227006\n",
      "Iteration 282, loss = 0.13072074\n",
      "Iteration 283, loss = 0.12919140\n",
      "Iteration 284, loss = 0.12765801\n",
      "Iteration 285, loss = 0.12586979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 286, loss = 0.12404935\n",
      "Iteration 287, loss = 0.12213901\n",
      "Iteration 288, loss = 0.11998023\n",
      "Iteration 289, loss = 0.11776270\n",
      "Iteration 290, loss = 0.11549999\n",
      "Iteration 291, loss = 0.11307717\n",
      "Iteration 292, loss = 0.11035862\n",
      "Iteration 293, loss = 0.10737370\n",
      "Iteration 294, loss = 0.10466704\n",
      "Iteration 295, loss = 0.10212777\n",
      "Iteration 296, loss = 0.09969209\n",
      "Iteration 297, loss = 0.09707169\n",
      "Iteration 298, loss = 0.09502276\n",
      "Iteration 299, loss = 0.09367969\n",
      "Iteration 300, loss = 0.09244396\n",
      "Iteration 301, loss = 0.09148271\n",
      "Iteration 302, loss = 0.09064797\n",
      "Iteration 303, loss = 0.08995363\n",
      "Iteration 304, loss = 0.08925462\n",
      "Iteration 305, loss = 0.08873455\n",
      "Iteration 306, loss = 0.08820197\n",
      "Iteration 307, loss = 0.08776667\n",
      "Iteration 308, loss = 0.08740394\n",
      "Iteration 309, loss = 0.08704164\n",
      "Iteration 310, loss = 0.08676683\n",
      "Iteration 311, loss = 0.08641961\n",
      "Iteration 312, loss = 0.08612121\n",
      "Iteration 313, loss = 0.08583349\n",
      "Iteration 314, loss = 0.08553757\n",
      "Iteration 315, loss = 0.08525383\n",
      "Iteration 316, loss = 0.08497355\n",
      "Iteration 317, loss = 0.08470566\n",
      "Iteration 318, loss = 0.08443907\n",
      "Iteration 319, loss = 0.08415425\n",
      "Iteration 320, loss = 0.08391489\n",
      "Iteration 321, loss = 0.08364631\n",
      "Iteration 322, loss = 0.08341481\n",
      "Iteration 323, loss = 0.08316851\n",
      "Iteration 324, loss = 0.08293932\n",
      "Iteration 325, loss = 0.08268541\n",
      "Iteration 326, loss = 0.08245979\n",
      "Iteration 327, loss = 0.08223380\n",
      "Iteration 328, loss = 0.08202642\n",
      "Iteration 329, loss = 0.08184081\n",
      "Iteration 330, loss = 0.08156485\n",
      "Iteration 331, loss = 0.08134772\n",
      "Iteration 332, loss = 0.08112952\n",
      "Iteration 333, loss = 0.08092416\n",
      "Iteration 334, loss = 0.08070703\n",
      "Iteration 335, loss = 0.08051917\n",
      "Iteration 336, loss = 0.08029092\n",
      "Iteration 337, loss = 0.08008359\n",
      "Iteration 338, loss = 0.07989614\n",
      "Iteration 339, loss = 0.07966822\n",
      "Iteration 340, loss = 0.07946400\n",
      "Iteration 341, loss = 0.07927230\n",
      "Iteration 342, loss = 0.07911633\n",
      "Iteration 343, loss = 0.07888556\n",
      "Iteration 344, loss = 0.07867334\n",
      "Iteration 345, loss = 0.07847457\n",
      "Iteration 346, loss = 0.07829127\n",
      "Iteration 347, loss = 0.07810114\n",
      "Iteration 348, loss = 0.07790585\n",
      "Iteration 349, loss = 0.07770897\n",
      "Iteration 350, loss = 0.07753698\n",
      "Iteration 351, loss = 0.07734629\n",
      "Iteration 352, loss = 0.07716107\n",
      "Iteration 353, loss = 0.07700060\n",
      "Iteration 354, loss = 0.07678046\n",
      "Iteration 355, loss = 0.07659497\n",
      "Iteration 356, loss = 0.07641700\n",
      "Iteration 357, loss = 0.07623531\n",
      "Iteration 358, loss = 0.07605921\n",
      "Iteration 359, loss = 0.07588380\n",
      "Iteration 360, loss = 0.07573819\n",
      "Iteration 361, loss = 0.07554443\n",
      "Iteration 362, loss = 0.07536158\n",
      "Iteration 363, loss = 0.07515584\n",
      "Iteration 364, loss = 0.07498549\n",
      "Iteration 365, loss = 0.07480585\n",
      "Iteration 366, loss = 0.07463382\n",
      "Iteration 367, loss = 0.07444152\n",
      "Iteration 368, loss = 0.07426626\n",
      "Iteration 369, loss = 0.07410714\n",
      "Iteration 370, loss = 0.07390814\n",
      "Iteration 371, loss = 0.07373148\n",
      "Iteration 372, loss = 0.07359456\n",
      "Iteration 373, loss = 0.07339448\n",
      "Iteration 374, loss = 0.07324382\n",
      "Iteration 375, loss = 0.07304186\n",
      "Iteration 376, loss = 0.07288400\n",
      "Iteration 377, loss = 0.07273099\n",
      "Iteration 378, loss = 0.07256392\n",
      "Iteration 379, loss = 0.07238266\n",
      "Iteration 380, loss = 0.07221550\n",
      "Iteration 381, loss = 0.07203367\n",
      "Iteration 382, loss = 0.07185907\n",
      "Iteration 383, loss = 0.07170022\n",
      "Iteration 384, loss = 0.07152695\n",
      "Iteration 385, loss = 0.07137166\n",
      "Iteration 386, loss = 0.07119154\n",
      "Iteration 387, loss = 0.07105995\n",
      "Iteration 388, loss = 0.07085932\n",
      "Iteration 389, loss = 0.07069147\n",
      "Iteration 390, loss = 0.07054356\n",
      "Iteration 391, loss = 0.07036403\n",
      "Iteration 392, loss = 0.07019868\n",
      "Iteration 393, loss = 0.07004124\n",
      "Iteration 394, loss = 0.06988885\n",
      "Iteration 395, loss = 0.06973329\n",
      "Iteration 396, loss = 0.06954598\n",
      "Iteration 397, loss = 0.06937872\n",
      "Iteration 398, loss = 0.06923307\n",
      "Iteration 399, loss = 0.06908137\n",
      "Iteration 400, loss = 0.06889457\n",
      "Iteration 401, loss = 0.06874157\n",
      "Iteration 402, loss = 0.06860677\n",
      "Iteration 403, loss = 0.06843811\n",
      "Iteration 404, loss = 0.06825486\n",
      "Iteration 405, loss = 0.06809012\n",
      "Iteration 406, loss = 0.06792599\n",
      "Iteration 407, loss = 0.06778094\n",
      "Iteration 408, loss = 0.06761585\n",
      "Iteration 409, loss = 0.06745872\n",
      "Iteration 410, loss = 0.06733077\n",
      "Iteration 411, loss = 0.06716410\n",
      "Iteration 412, loss = 0.06699535\n",
      "Iteration 413, loss = 0.06683009\n",
      "Iteration 414, loss = 0.06665752\n",
      "Iteration 415, loss = 0.06653831\n",
      "Iteration 416, loss = 0.06635959\n",
      "Iteration 417, loss = 0.06623421\n",
      "Iteration 418, loss = 0.06604566\n",
      "Iteration 419, loss = 0.06589151\n",
      "Iteration 420, loss = 0.06575108\n",
      "Iteration 421, loss = 0.06557688\n",
      "Iteration 422, loss = 0.06543634\n",
      "Iteration 423, loss = 0.06527812\n",
      "Iteration 424, loss = 0.06514294\n",
      "Iteration 425, loss = 0.06495158\n",
      "Iteration 426, loss = 0.06481697\n",
      "Iteration 427, loss = 0.06466609\n",
      "Iteration 428, loss = 0.06453589\n",
      "Iteration 429, loss = 0.06434601\n",
      "Iteration 430, loss = 0.06421357\n",
      "Iteration 431, loss = 0.06404675\n",
      "Iteration 432, loss = 0.06392815\n",
      "Iteration 433, loss = 0.06377134\n",
      "Iteration 434, loss = 0.06362062\n",
      "Iteration 435, loss = 0.06343383\n",
      "Iteration 436, loss = 0.06328806\n",
      "Iteration 437, loss = 0.06312668\n",
      "Iteration 438, loss = 0.06298193\n",
      "Iteration 439, loss = 0.06283829\n",
      "Iteration 440, loss = 0.06268547\n",
      "Iteration 441, loss = 0.06255617\n",
      "Iteration 442, loss = 0.06238986\n",
      "Iteration 443, loss = 0.06224383\n",
      "Iteration 444, loss = 0.06208213\n",
      "Iteration 445, loss = 0.06195453\n",
      "Iteration 446, loss = 0.06177798\n",
      "Iteration 447, loss = 0.06165208\n",
      "Iteration 448, loss = 0.06147721\n",
      "Iteration 449, loss = 0.06135097\n",
      "Iteration 450, loss = 0.06118541\n",
      "Iteration 451, loss = 0.06102948\n",
      "Iteration 452, loss = 0.06091480\n",
      "Iteration 453, loss = 0.06075108\n",
      "Iteration 454, loss = 0.06060092\n",
      "Iteration 455, loss = 0.06044343\n",
      "Iteration 456, loss = 0.06033756\n",
      "Iteration 457, loss = 0.06020101\n",
      "Iteration 458, loss = 0.05998890\n",
      "Iteration 459, loss = 0.05987776\n",
      "Iteration 460, loss = 0.05972654\n",
      "Iteration 461, loss = 0.05957062\n",
      "Iteration 462, loss = 0.05945666\n",
      "Iteration 463, loss = 0.05926722\n",
      "Iteration 464, loss = 0.05913362\n",
      "Iteration 465, loss = 0.05900578\n",
      "Iteration 466, loss = 0.05887067\n",
      "Iteration 467, loss = 0.05870204\n",
      "Iteration 468, loss = 0.05856837\n",
      "Iteration 469, loss = 0.05841192\n",
      "Iteration 470, loss = 0.05826973\n",
      "Iteration 471, loss = 0.05818246\n",
      "Iteration 472, loss = 0.05802567\n",
      "Iteration 473, loss = 0.05783938\n",
      "Iteration 474, loss = 0.05768932\n",
      "Iteration 475, loss = 0.05755961\n",
      "Iteration 476, loss = 0.05741577\n",
      "Iteration 477, loss = 0.05730734\n",
      "Iteration 478, loss = 0.05715782\n",
      "Iteration 479, loss = 0.05700067\n",
      "Iteration 480, loss = 0.05683519\n",
      "Iteration 481, loss = 0.05672394\n",
      "Iteration 482, loss = 0.05658465\n",
      "Iteration 483, loss = 0.05644297\n",
      "Iteration 484, loss = 0.05632490\n",
      "Iteration 485, loss = 0.05614608\n",
      "Iteration 486, loss = 0.05600896\n",
      "Iteration 487, loss = 0.05587329\n",
      "Iteration 488, loss = 0.05573847\n",
      "Iteration 489, loss = 0.05558129\n",
      "Iteration 490, loss = 0.05552360\n",
      "Iteration 491, loss = 0.05533757\n",
      "Iteration 492, loss = 0.05516695\n",
      "Iteration 493, loss = 0.05503634\n",
      "Iteration 494, loss = 0.05491430\n",
      "Iteration 495, loss = 0.05477956\n",
      "Iteration 496, loss = 0.05462942\n",
      "Iteration 497, loss = 0.05450036\n",
      "Iteration 498, loss = 0.05435984\n",
      "Iteration 499, loss = 0.05420333\n",
      "Iteration 500, loss = 0.05410169\n",
      "Iteration 501, loss = 0.05394725\n",
      "Iteration 502, loss = 0.05381438\n",
      "Iteration 503, loss = 0.05366778\n",
      "Iteration 504, loss = 0.05355712\n",
      "Iteration 505, loss = 0.05340035\n",
      "Iteration 506, loss = 0.05327993\n",
      "Iteration 507, loss = 0.05313189\n",
      "Iteration 508, loss = 0.05304129\n",
      "Iteration 509, loss = 0.05290608\n",
      "Iteration 510, loss = 0.05276085\n",
      "Iteration 511, loss = 0.05259594\n",
      "Iteration 512, loss = 0.05250267\n",
      "Iteration 513, loss = 0.05233612\n",
      "Iteration 514, loss = 0.05221489\n",
      "Iteration 515, loss = 0.05207781\n",
      "Iteration 516, loss = 0.05194005\n",
      "Iteration 517, loss = 0.05181836\n",
      "Iteration 518, loss = 0.05167451\n",
      "Iteration 519, loss = 0.05155131\n",
      "Iteration 520, loss = 0.05141482\n",
      "Iteration 521, loss = 0.05128062\n",
      "Iteration 522, loss = 0.05115066\n",
      "Iteration 523, loss = 0.05102366\n",
      "Iteration 524, loss = 0.05088225\n",
      "Iteration 525, loss = 0.05079997\n",
      "Iteration 526, loss = 0.05061894\n",
      "Iteration 527, loss = 0.05049401\n",
      "Iteration 528, loss = 0.05035641\n",
      "Iteration 529, loss = 0.05026095\n",
      "Iteration 530, loss = 0.05012125\n",
      "Iteration 531, loss = 0.04994772\n",
      "Iteration 532, loss = 0.04984388\n",
      "Iteration 533, loss = 0.04975059\n",
      "Iteration 534, loss = 0.04962228\n",
      "Iteration 535, loss = 0.04948366\n",
      "Iteration 536, loss = 0.04932178\n",
      "Iteration 537, loss = 0.04919433\n",
      "Iteration 538, loss = 0.04904643\n",
      "Iteration 539, loss = 0.04893408\n",
      "Iteration 540, loss = 0.04879868\n",
      "Iteration 541, loss = 0.04867741\n",
      "Iteration 542, loss = 0.04855310\n",
      "Iteration 543, loss = 0.04841337\n",
      "Iteration 544, loss = 0.04828366\n",
      "Iteration 545, loss = 0.04815989\n",
      "Iteration 546, loss = 0.04803822\n",
      "Iteration 547, loss = 0.04791304\n",
      "Iteration 548, loss = 0.04777817\n",
      "Iteration 549, loss = 0.04767177\n",
      "Iteration 550, loss = 0.04754260\n",
      "Iteration 551, loss = 0.04740310\n",
      "Iteration 552, loss = 0.04730404\n",
      "Iteration 553, loss = 0.04716042\n",
      "Iteration 554, loss = 0.04702207\n",
      "Iteration 555, loss = 0.04692613\n",
      "Iteration 556, loss = 0.04677605\n",
      "Iteration 557, loss = 0.04666207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 558, loss = 0.04651852\n",
      "Iteration 559, loss = 0.04641412\n",
      "Iteration 560, loss = 0.04628078\n",
      "Iteration 561, loss = 0.04616193\n",
      "Iteration 562, loss = 0.04603530\n",
      "Iteration 563, loss = 0.04592328\n",
      "Iteration 564, loss = 0.04581171\n",
      "Iteration 565, loss = 0.04567883\n",
      "Iteration 566, loss = 0.04554445\n",
      "Iteration 567, loss = 0.04543601\n",
      "Iteration 568, loss = 0.04529043\n",
      "Iteration 569, loss = 0.04517432\n",
      "Iteration 570, loss = 0.04505263\n",
      "Iteration 571, loss = 0.04495867\n",
      "Iteration 572, loss = 0.04482715\n",
      "Iteration 573, loss = 0.04470361\n",
      "Iteration 574, loss = 0.04457222\n",
      "Iteration 575, loss = 0.04444715\n",
      "Iteration 576, loss = 0.04432785\n",
      "Iteration 577, loss = 0.04420175\n",
      "Iteration 578, loss = 0.04407682\n",
      "Iteration 579, loss = 0.04401783\n",
      "Iteration 580, loss = 0.04388751\n",
      "Iteration 581, loss = 0.04371417\n",
      "Iteration 582, loss = 0.04358027\n",
      "Iteration 583, loss = 0.04347697\n",
      "Iteration 584, loss = 0.04334234\n",
      "Iteration 585, loss = 0.04323888\n",
      "Iteration 586, loss = 0.04312582\n",
      "Iteration 587, loss = 0.04302275\n",
      "Iteration 588, loss = 0.04288984\n",
      "Iteration 589, loss = 0.04277164\n",
      "Iteration 590, loss = 0.04264846\n",
      "Iteration 591, loss = 0.04255515\n",
      "Iteration 592, loss = 0.04244392\n",
      "Iteration 593, loss = 0.04234379\n",
      "Iteration 594, loss = 0.04219807\n",
      "Iteration 595, loss = 0.04206353\n",
      "Iteration 596, loss = 0.04195393\n",
      "Iteration 597, loss = 0.04185454\n",
      "Iteration 598, loss = 0.04172964\n",
      "Iteration 599, loss = 0.04162209\n",
      "Iteration 600, loss = 0.04150232\n",
      "Iteration 601, loss = 0.04143691\n",
      "Iteration 602, loss = 0.04128715\n",
      "Iteration 603, loss = 0.04116011\n",
      "Iteration 604, loss = 0.04104842\n",
      "Iteration 605, loss = 0.04093427\n",
      "Iteration 606, loss = 0.04083774\n",
      "Iteration 607, loss = 0.04070632\n",
      "Iteration 608, loss = 0.04059995\n",
      "Iteration 609, loss = 0.04050383\n",
      "Iteration 610, loss = 0.04036709\n",
      "Iteration 611, loss = 0.04029554\n",
      "Iteration 612, loss = 0.04014839\n",
      "Iteration 613, loss = 0.04004236\n",
      "Iteration 614, loss = 0.03998436\n",
      "Iteration 615, loss = 0.03981109\n",
      "Iteration 616, loss = 0.03971687\n",
      "Iteration 617, loss = 0.03959127\n",
      "Iteration 618, loss = 0.03948754\n",
      "Iteration 619, loss = 0.03938736\n",
      "Iteration 620, loss = 0.03927830\n",
      "Iteration 621, loss = 0.03916143\n",
      "Iteration 622, loss = 0.03905045\n",
      "Iteration 623, loss = 0.03893382\n",
      "Iteration 624, loss = 0.03883475\n",
      "Iteration 625, loss = 0.03871990\n",
      "Iteration 626, loss = 0.03867440\n",
      "Iteration 627, loss = 0.03849487\n",
      "Iteration 628, loss = 0.03840574\n",
      "Iteration 629, loss = 0.03829980\n",
      "Iteration 630, loss = 0.03819909\n",
      "Iteration 631, loss = 0.03811117\n",
      "Iteration 632, loss = 0.03798533\n",
      "Iteration 633, loss = 0.03786396\n",
      "Iteration 634, loss = 0.03776702\n",
      "Iteration 635, loss = 0.03766563\n",
      "Iteration 636, loss = 0.03755686\n",
      "Iteration 637, loss = 0.03744416\n",
      "Iteration 638, loss = 0.03732618\n",
      "Iteration 639, loss = 0.03723161\n",
      "Iteration 640, loss = 0.03715913\n",
      "Iteration 641, loss = 0.03707495\n",
      "Iteration 642, loss = 0.03694865\n",
      "Iteration 643, loss = 0.03683586\n",
      "Iteration 644, loss = 0.03671675\n",
      "Iteration 645, loss = 0.03662502\n",
      "Iteration 646, loss = 0.03651579\n",
      "Iteration 647, loss = 0.03643620\n",
      "Iteration 648, loss = 0.03632579\n",
      "Iteration 649, loss = 0.03626269\n",
      "Iteration 650, loss = 0.03612982\n",
      "Iteration 651, loss = 0.03600496\n",
      "Iteration 652, loss = 0.03588384\n",
      "Iteration 653, loss = 0.03580604\n",
      "Iteration 654, loss = 0.03570362\n",
      "Iteration 655, loss = 0.03560003\n",
      "Iteration 656, loss = 0.03553606\n",
      "Iteration 657, loss = 0.03541091\n",
      "Iteration 658, loss = 0.03529727\n",
      "Iteration 659, loss = 0.03519929\n",
      "Iteration 660, loss = 0.03509437\n",
      "Iteration 661, loss = 0.03499522\n",
      "Iteration 662, loss = 0.03496954\n",
      "Iteration 663, loss = 0.03479400\n",
      "Iteration 664, loss = 0.03469544\n",
      "Iteration 665, loss = 0.03458952\n",
      "Iteration 666, loss = 0.03448112\n",
      "Iteration 667, loss = 0.03438800\n",
      "Iteration 668, loss = 0.03427714\n",
      "Iteration 669, loss = 0.03419192\n",
      "Iteration 670, loss = 0.03411644\n",
      "Iteration 671, loss = 0.03399976\n",
      "Iteration 672, loss = 0.03390863\n",
      "Iteration 673, loss = 0.03383849\n",
      "Iteration 674, loss = 0.03371707\n",
      "Iteration 675, loss = 0.03360479\n",
      "Iteration 676, loss = 0.03350321\n",
      "Iteration 677, loss = 0.03343282\n",
      "Iteration 678, loss = 0.03332153\n",
      "Iteration 679, loss = 0.03322852\n",
      "Iteration 680, loss = 0.03313692\n",
      "Iteration 681, loss = 0.03304393\n",
      "Iteration 682, loss = 0.03293258\n",
      "Iteration 683, loss = 0.03284819\n",
      "Iteration 684, loss = 0.03280579\n",
      "Iteration 685, loss = 0.03265518\n",
      "Iteration 686, loss = 0.03255444\n",
      "Iteration 687, loss = 0.03245939\n",
      "Iteration 688, loss = 0.03237054\n",
      "Iteration 689, loss = 0.03232272\n",
      "Iteration 690, loss = 0.03219278\n",
      "Iteration 691, loss = 0.03207835\n",
      "Iteration 692, loss = 0.03200626\n",
      "Iteration 693, loss = 0.03189612\n",
      "Iteration 694, loss = 0.03181266\n",
      "Iteration 695, loss = 0.03173440\n",
      "Iteration 696, loss = 0.03165390\n",
      "Iteration 697, loss = 0.03153153\n",
      "Iteration 698, loss = 0.03147635\n",
      "Iteration 699, loss = 0.03135193\n",
      "Iteration 700, loss = 0.03126427\n",
      "Iteration 701, loss = 0.03119758\n",
      "Iteration 702, loss = 0.03106476\n",
      "Iteration 703, loss = 0.03098036\n",
      "Iteration 704, loss = 0.03089742\n",
      "Iteration 705, loss = 0.03080288\n",
      "Iteration 706, loss = 0.03071836\n",
      "Iteration 707, loss = 0.03060852\n",
      "Iteration 708, loss = 0.03052781\n",
      "Iteration 709, loss = 0.03045212\n",
      "Iteration 710, loss = 0.03035497\n",
      "Iteration 711, loss = 0.03024805\n",
      "Iteration 712, loss = 0.03017933\n",
      "Iteration 713, loss = 0.03009028\n",
      "Iteration 714, loss = 0.02999128\n",
      "Iteration 715, loss = 0.02990184\n",
      "Iteration 716, loss = 0.02980974\n",
      "Iteration 717, loss = 0.02974533\n",
      "Iteration 718, loss = 0.02965688\n",
      "Iteration 719, loss = 0.02957201\n",
      "Iteration 720, loss = 0.02946592\n",
      "Iteration 721, loss = 0.02938427\n",
      "Iteration 722, loss = 0.02927960\n",
      "Iteration 723, loss = 0.02920992\n",
      "Iteration 724, loss = 0.02912116\n",
      "Iteration 725, loss = 0.02905256\n",
      "Iteration 726, loss = 0.02896586\n",
      "Iteration 727, loss = 0.02888223\n",
      "Iteration 728, loss = 0.02877691\n",
      "Iteration 729, loss = 0.02868905\n",
      "Iteration 730, loss = 0.02862318\n",
      "Iteration 731, loss = 0.02855475\n",
      "Iteration 732, loss = 0.02843580\n",
      "Iteration 733, loss = 0.02838441\n",
      "Iteration 734, loss = 0.02827209\n",
      "Iteration 735, loss = 0.02819803\n",
      "Iteration 736, loss = 0.02812407\n",
      "Iteration 737, loss = 0.02800753\n",
      "Iteration 738, loss = 0.02793301\n",
      "Iteration 739, loss = 0.02784045\n",
      "Iteration 740, loss = 0.02778126\n",
      "Iteration 741, loss = 0.02769013\n",
      "Iteration 742, loss = 0.02763534\n",
      "Iteration 743, loss = 0.02756202\n",
      "Iteration 744, loss = 0.02742689\n",
      "Iteration 745, loss = 0.02738474\n",
      "Iteration 746, loss = 0.02729701\n",
      "Iteration 747, loss = 0.02720616\n",
      "Iteration 748, loss = 0.02710955\n",
      "Iteration 749, loss = 0.02704465\n",
      "Iteration 750, loss = 0.02694893\n",
      "Iteration 751, loss = 0.02687801\n",
      "Iteration 752, loss = 0.02678991\n",
      "Iteration 753, loss = 0.02670720\n",
      "Iteration 754, loss = 0.02666168\n",
      "Iteration 755, loss = 0.02656295\n",
      "Iteration 756, loss = 0.02648760\n",
      "Iteration 757, loss = 0.02638873\n",
      "Iteration 758, loss = 0.02631269\n",
      "Iteration 759, loss = 0.02623527\n",
      "Iteration 760, loss = 0.02616410\n",
      "Iteration 761, loss = 0.02610359\n",
      "Iteration 762, loss = 0.02602803\n",
      "Iteration 763, loss = 0.02596423\n",
      "Iteration 764, loss = 0.02591252\n",
      "Iteration 765, loss = 0.02582981\n",
      "Iteration 766, loss = 0.02572249\n",
      "Iteration 767, loss = 0.02565430\n",
      "Iteration 768, loss = 0.02555085\n",
      "Iteration 769, loss = 0.02547540\n",
      "Iteration 770, loss = 0.02539002\n",
      "Iteration 771, loss = 0.02532562\n",
      "Iteration 772, loss = 0.02524867\n",
      "Iteration 773, loss = 0.02518330\n",
      "Iteration 774, loss = 0.02510458\n",
      "Iteration 775, loss = 0.02505233\n",
      "Iteration 776, loss = 0.02495134\n",
      "Iteration 777, loss = 0.02485978\n",
      "Iteration 778, loss = 0.02479948\n",
      "Iteration 779, loss = 0.02472351\n",
      "Iteration 780, loss = 0.02465582\n",
      "Iteration 781, loss = 0.02456106\n",
      "Iteration 782, loss = 0.02450869\n",
      "Iteration 783, loss = 0.02444466\n",
      "Iteration 784, loss = 0.02434818\n",
      "Iteration 785, loss = 0.02429143\n",
      "Iteration 786, loss = 0.02423198\n",
      "Iteration 787, loss = 0.02414127\n",
      "Iteration 788, loss = 0.02405152\n",
      "Iteration 789, loss = 0.02400130\n",
      "Iteration 790, loss = 0.02392841\n",
      "Iteration 791, loss = 0.02386973\n",
      "Iteration 792, loss = 0.02379606\n",
      "Iteration 793, loss = 0.02373242\n",
      "Iteration 794, loss = 0.02363429\n",
      "Iteration 795, loss = 0.02356336\n",
      "Iteration 796, loss = 0.02349905\n",
      "Iteration 797, loss = 0.02343010\n",
      "Iteration 798, loss = 0.02333855\n",
      "Iteration 799, loss = 0.02329964\n",
      "Iteration 800, loss = 0.02322458\n",
      "Iteration 801, loss = 0.02315523\n",
      "Iteration 802, loss = 0.02306385\n",
      "Iteration 803, loss = 0.02302628\n",
      "Iteration 804, loss = 0.02299733\n",
      "Iteration 805, loss = 0.02284751\n",
      "Iteration 806, loss = 0.02282611\n",
      "Iteration 807, loss = 0.02271875\n",
      "Iteration 808, loss = 0.02269464\n",
      "Iteration 809, loss = 0.02261299\n",
      "Iteration 810, loss = 0.02254417\n",
      "Iteration 811, loss = 0.02245165\n",
      "Iteration 812, loss = 0.02242191\n",
      "Iteration 813, loss = 0.02230037\n",
      "Iteration 814, loss = 0.02223975\n",
      "Iteration 815, loss = 0.02222573\n",
      "Iteration 816, loss = 0.02214278\n",
      "Iteration 817, loss = 0.02205175\n",
      "Iteration 818, loss = 0.02198801\n",
      "Iteration 819, loss = 0.02192884\n",
      "Iteration 820, loss = 0.02186802\n",
      "Iteration 821, loss = 0.02178703\n",
      "Iteration 822, loss = 0.02173470\n",
      "Iteration 823, loss = 0.02166911\n",
      "Iteration 824, loss = 0.02163889\n",
      "Iteration 825, loss = 0.02156566\n",
      "Iteration 826, loss = 0.02151251\n",
      "Iteration 827, loss = 0.02142261\n",
      "Iteration 828, loss = 0.02131834\n",
      "Iteration 829, loss = 0.02125504\n",
      "Iteration 830, loss = 0.02119326\n",
      "Iteration 831, loss = 0.02117485\n",
      "Iteration 832, loss = 0.02108684\n",
      "Iteration 833, loss = 0.02099928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 834, loss = 0.02098350\n",
      "Iteration 835, loss = 0.02087345\n",
      "Iteration 836, loss = 0.02081314\n",
      "Iteration 837, loss = 0.02074665\n",
      "Iteration 838, loss = 0.02072157\n",
      "Iteration 839, loss = 0.02062224\n",
      "Iteration 840, loss = 0.02055609\n",
      "Iteration 841, loss = 0.02052233\n",
      "Iteration 842, loss = 0.02045123\n",
      "Iteration 843, loss = 0.02039781\n",
      "Iteration 844, loss = 0.02033019\n",
      "Iteration 845, loss = 0.02027055\n",
      "Iteration 846, loss = 0.02019296\n",
      "Iteration 847, loss = 0.02015044\n",
      "Iteration 848, loss = 0.02008489\n",
      "Iteration 849, loss = 0.02002279\n",
      "Iteration 850, loss = 0.01999708\n",
      "Iteration 851, loss = 0.01990122\n",
      "Iteration 852, loss = 0.01985388\n",
      "Iteration 853, loss = 0.01981694\n",
      "Iteration 854, loss = 0.01976712\n",
      "Iteration 855, loss = 0.01966482\n",
      "Iteration 856, loss = 0.01961539\n",
      "Iteration 857, loss = 0.01954380\n",
      "Iteration 858, loss = 0.01948783\n",
      "Iteration 859, loss = 0.01943486\n",
      "Iteration 860, loss = 0.01940912\n",
      "Iteration 861, loss = 0.01933764\n",
      "Iteration 862, loss = 0.01927573\n",
      "Iteration 863, loss = 0.01922041\n",
      "Iteration 864, loss = 0.01914188\n",
      "Iteration 865, loss = 0.01910542\n",
      "Iteration 866, loss = 0.01910973\n",
      "Iteration 867, loss = 0.01901750\n",
      "Iteration 868, loss = 0.01895559\n",
      "Iteration 869, loss = 0.01886297\n",
      "Iteration 870, loss = 0.01882496\n",
      "Iteration 871, loss = 0.01874831\n",
      "Iteration 872, loss = 0.01873440\n",
      "Iteration 873, loss = 0.01865690\n",
      "Iteration 874, loss = 0.01860514\n",
      "Iteration 875, loss = 0.01860017\n",
      "Iteration 876, loss = 0.01847217\n",
      "Iteration 877, loss = 0.01845677\n",
      "Iteration 878, loss = 0.01839950\n",
      "Iteration 879, loss = 0.01832675\n",
      "Iteration 880, loss = 0.01827315\n",
      "Iteration 881, loss = 0.01824357\n",
      "Iteration 882, loss = 0.01814551\n",
      "Iteration 883, loss = 0.01809280\n",
      "Iteration 884, loss = 0.01805289\n",
      "Iteration 885, loss = 0.01800402\n",
      "Iteration 886, loss = 0.01798010\n",
      "Iteration 887, loss = 0.01786580\n",
      "Iteration 888, loss = 0.01786823\n",
      "Iteration 889, loss = 0.01779426\n",
      "Iteration 890, loss = 0.01772484\n",
      "Iteration 891, loss = 0.01769681\n",
      "Iteration 892, loss = 0.01762309\n",
      "Iteration 893, loss = 0.01757310\n",
      "Iteration 894, loss = 0.01755894\n",
      "Iteration 895, loss = 0.01748456\n",
      "Iteration 896, loss = 0.01750082\n",
      "Iteration 897, loss = 0.01742707\n",
      "Iteration 898, loss = 0.01730793\n",
      "Iteration 899, loss = 0.01726004\n",
      "Iteration 900, loss = 0.01722490\n",
      "Iteration 901, loss = 0.01717807\n",
      "Iteration 902, loss = 0.01713711\n",
      "Iteration 903, loss = 0.01705226\n",
      "Iteration 904, loss = 0.01700276\n",
      "Iteration 905, loss = 0.01695347\n",
      "Iteration 906, loss = 0.01694510\n",
      "Iteration 907, loss = 0.01688005\n",
      "Iteration 908, loss = 0.01682739\n",
      "Iteration 909, loss = 0.01679132\n",
      "Iteration 910, loss = 0.01674724\n",
      "Iteration 911, loss = 0.01673612\n",
      "Iteration 912, loss = 0.01662809\n",
      "Iteration 913, loss = 0.01655407\n",
      "Iteration 914, loss = 0.01654509\n",
      "Iteration 915, loss = 0.01647235\n",
      "Iteration 916, loss = 0.01643484\n",
      "Iteration 917, loss = 0.01636524\n",
      "Iteration 918, loss = 0.01633474\n",
      "Iteration 919, loss = 0.01627622\n",
      "Iteration 920, loss = 0.01624114\n",
      "Iteration 921, loss = 0.01618773\n",
      "Iteration 922, loss = 0.01617321\n",
      "Iteration 923, loss = 0.01610227\n",
      "Iteration 924, loss = 0.01606265\n",
      "Iteration 925, loss = 0.01601244\n",
      "Iteration 926, loss = 0.01596848\n",
      "Iteration 927, loss = 0.01590988\n",
      "Iteration 928, loss = 0.01585717\n",
      "Iteration 929, loss = 0.01579799\n",
      "Iteration 930, loss = 0.01576481\n",
      "Iteration 931, loss = 0.01571893\n",
      "Iteration 932, loss = 0.01567116\n",
      "Iteration 933, loss = 0.01564766\n",
      "Iteration 934, loss = 0.01558319\n",
      "Iteration 935, loss = 0.01551339\n",
      "Iteration 936, loss = 0.01550140\n",
      "Iteration 937, loss = 0.01543458\n",
      "Iteration 938, loss = 0.01537890\n",
      "Iteration 939, loss = 0.01537729\n",
      "Iteration 940, loss = 0.01531332\n",
      "Iteration 941, loss = 0.01526870\n",
      "Iteration 942, loss = 0.01523817\n",
      "Iteration 943, loss = 0.01519103\n",
      "Iteration 944, loss = 0.01511528\n",
      "Iteration 945, loss = 0.01507270\n",
      "Iteration 946, loss = 0.01503587\n",
      "Iteration 947, loss = 0.01501901\n",
      "Iteration 948, loss = 0.01495106\n",
      "Iteration 949, loss = 0.01490767\n",
      "Iteration 950, loss = 0.01487291\n",
      "Iteration 951, loss = 0.01488549\n",
      "Iteration 952, loss = 0.01478255\n",
      "Iteration 953, loss = 0.01476882\n",
      "Iteration 954, loss = 0.01469070\n",
      "Iteration 955, loss = 0.01467812\n",
      "Iteration 956, loss = 0.01463566\n",
      "Iteration 957, loss = 0.01456950\n",
      "Iteration 958, loss = 0.01454461\n",
      "Iteration 959, loss = 0.01448432\n",
      "Iteration 960, loss = 0.01445543\n",
      "Iteration 961, loss = 0.01443879\n",
      "Iteration 962, loss = 0.01434821\n",
      "Iteration 963, loss = 0.01433504\n",
      "Iteration 964, loss = 0.01428222\n",
      "Iteration 965, loss = 0.01425284\n",
      "Iteration 966, loss = 0.01422860\n",
      "Iteration 967, loss = 0.01416958\n",
      "Iteration 968, loss = 0.01415079\n",
      "Iteration 969, loss = 0.01406450\n",
      "Iteration 970, loss = 0.01404603\n",
      "Iteration 971, loss = 0.01402795\n",
      "Iteration 972, loss = 0.01397536\n",
      "Iteration 973, loss = 0.01391839\n",
      "Iteration 974, loss = 0.01388987\n",
      "Iteration 975, loss = 0.01381589\n",
      "Iteration 976, loss = 0.01380085\n",
      "Iteration 977, loss = 0.01374888\n",
      "Iteration 978, loss = 0.01371880\n",
      "Iteration 979, loss = 0.01369017\n",
      "Iteration 980, loss = 0.01362976\n",
      "Iteration 981, loss = 0.01363058\n",
      "Iteration 982, loss = 0.01356000\n",
      "Iteration 983, loss = 0.01350509\n",
      "Iteration 984, loss = 0.01347357\n",
      "Iteration 985, loss = 0.01348752\n",
      "Iteration 986, loss = 0.01344966\n",
      "Iteration 987, loss = 0.01336301\n",
      "Iteration 988, loss = 0.01331909\n",
      "Iteration 989, loss = 0.01327295\n",
      "Iteration 990, loss = 0.01323624\n",
      "Iteration 991, loss = 0.01321436\n",
      "Iteration 992, loss = 0.01317021\n",
      "Iteration 993, loss = 0.01315267\n",
      "Iteration 994, loss = 0.01310105\n",
      "Iteration 995, loss = 0.01306613\n",
      "Iteration 996, loss = 0.01309342\n",
      "Iteration 997, loss = 0.01305380\n",
      "Iteration 998, loss = 0.01295118\n",
      "Iteration 999, loss = 0.01292291\n",
      "Iteration 1000, loss = 0.01287909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeonelSilima\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1000, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_credit = MLPClassifier(max_iter=1000, verbose=True, tol=0.0000100, hidden_layer_sizes=(2,2,),  #Arquitetura da rede neural\n",
    "                                                    solver='adam', #aplica-se para bases de daados mais complexos\n",
    "                                                    activation='relu') # RELU arredonda para zero qualquer n negativo\n",
    "neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f98883f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsao = neural_credit.predict(X_credit_teste) # Analise de eficiencia com dados de teste\n",
    "previsao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5e66398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df0df130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e89a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix # Matrix de confusao\n",
    "cm =  ConfusionMatrix(neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f4c6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsao)) # Visualizacao da estatistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffb664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
